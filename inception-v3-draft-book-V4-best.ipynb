{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ca838",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-02T01:32:42.987817Z",
     "iopub.status.busy": "2023-06-02T01:32:42.987396Z",
     "iopub.status.idle": "2023-06-02T01:32:46.634622Z",
     "shell.execute_reply": "2023-06-02T01:32:46.633715Z"
    },
    "papermill": {
     "duration": 3.655592,
     "end_time": "2023-06-02T01:32:46.638145",
     "exception": false,
     "start_time": "2023-06-02T01:32:42.982553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # graphs\n",
    "\n",
    "import torch # tensors\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "checkpoints = '/kaggle/working/checkpoints/'\n",
    "if not os.path.exists(checkpoints):\n",
    "    os.makedirs(checkpoints)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029d654a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:32:46.646193Z",
     "iopub.status.busy": "2023-06-02T01:32:46.645644Z",
     "iopub.status.idle": "2023-06-02T01:33:06.495632Z",
     "shell.execute_reply": "2023-06-02T01:33:06.494635Z"
    },
    "papermill": {
     "duration": 19.856257,
     "end_time": "2023-06-02T01:33:06.498153",
     "exception": false,
     "start_time": "2023-06-02T01:32:46.641896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bird_data(augmentation=0):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.RandomCrop(299, padding=8, padding_mode='edge'), # Take 299x299 crops from padded images\n",
    "        transforms.RandomHorizontalFlip(),    # 50% of time flip image along y-axis\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23sp/birds/train', transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "#     # split our training data into a training set and a validation set, so I can tweak lr if needed and know model acc\n",
    "#     train_subset, val_subset = torch.utils.data.random_split(\n",
    "#         trainset, [.9, .1], generator=torch.Generator().manual_seed(455))\n",
    "#     # valloader will be input processed like the training set, that should be fine\n",
    "#     trainloader = torch.utils.data.DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=2)\n",
    "#     valloader = torch.utils.data.DataLoader(val_subset, batch_size=128, shuffle=True, num_workers=2\n",
    "#     , 'val': valloader \n",
    "    \n",
    "    testset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23sp/birds/test', transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
    "    classes = open(\"/kaggle/input/birds23sp/birds/names.txt\").read().strip().split(\"\\n\")\n",
    "    class_to_idx = trainset.class_to_idx\n",
    "    idx_to_class = {int(v): int(k) for k, v in class_to_idx.items()}\n",
    "    idx_to_name = {k: classes[v] for k,v in idx_to_class.items()}\n",
    "    return {'train': trainloader, 'test': testloader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n",
    "\n",
    "data = get_bird_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d38ac55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:33:06.506961Z",
     "iopub.status.busy": "2023-06-02T01:33:06.505399Z",
     "iopub.status.idle": "2023-06-02T01:33:06.519141Z",
     "shell.execute_reply": "2023-06-02T01:33:06.518139Z"
    },
    "papermill": {
     "duration": 0.019858,
     "end_time": "2023-06-02T01:33:06.521255",
     "exception": false,
     "start_time": "2023-06-02T01:33:06.501397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, dataloader, epochs=1, start_epoch=0, lr=0.01, momentum=0.9, decay=0.0005, \n",
    "          verbose=1, print_every=10, state=None, schedule={}, checkpoint_path=None):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    losses = []\n",
    "    vallosses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "\n",
    "    # Load previous training state\n",
    "    if state:\n",
    "        net.load_state_dict(state['net'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        start_epoch = state['epoch']\n",
    "        losses = state['losses']\n",
    "\n",
    "    # Fast forward lr schedule through already trained epochs\n",
    "    for epoch in range(start_epoch):\n",
    "        if epoch in schedule:\n",
    "            print (\"Learning rate: %f\"% schedule[epoch])\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = schedule[epoch]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        sum_loss = 0.0\n",
    "\n",
    "        # Update learning rate when scheduled\n",
    "        if epoch in schedule:\n",
    "            print (\"Learning rate: %f\"% schedule[epoch])\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = schedule[epoch]\n",
    "\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            # fetches the raw output, not the auxilory tensor. Inception net specific (on the [0])\n",
    "            loss = criterion(outputs[0], labels)\n",
    "            loss.backward()  # autograd magic, computes all the partial derivatives\n",
    "            optimizer.step() # takes a step in gradient direction\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if i % print_every == print_every-1:    # print every 10 mini-batches\n",
    "                if verbose:\n",
    "                  print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n",
    "                sum_loss = 0.0\n",
    "        if checkpoint_path:\n",
    "            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n",
    "            torch.save(state, checkpoint_path + 'checkpoint-%d.pkl'%(epoch+1))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126126b7",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-06-02T01:33:06.527901Z",
     "iopub.status.busy": "2023-06-02T01:33:06.527582Z",
     "iopub.status.idle": "2023-06-02T02:39:26.187394Z",
     "shell.execute_reply": "2023-06-02T02:39:26.186165Z"
    },
    "papermill": {
     "duration": 3979.667901,
     "end_time": "2023-06-02T02:39:26.191959",
     "exception": false,
     "start_time": "2023-06-02T01:33:06.524058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:02<00:00, 44.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.010000\n",
      "[0,    10] loss: 6.345\n",
      "[0,    20] loss: 6.282\n",
      "[0,    30] loss: 6.187\n",
      "[0,    40] loss: 6.061\n",
      "[0,    50] loss: 5.971\n",
      "[0,    60] loss: 5.826\n",
      "[0,    70] loss: 5.642\n",
      "[0,    80] loss: 5.467\n",
      "[0,    90] loss: 5.262\n",
      "[0,   100] loss: 5.140\n",
      "[0,   110] loss: 4.845\n",
      "[0,   120] loss: 4.692\n",
      "[0,   130] loss: 4.564\n",
      "[0,   140] loss: 4.371\n",
      "[0,   150] loss: 4.202\n",
      "[0,   160] loss: 4.026\n",
      "[0,   170] loss: 3.948\n",
      "[0,   180] loss: 3.800\n",
      "[0,   190] loss: 3.682\n",
      "[0,   200] loss: 3.591\n",
      "[0,   210] loss: 3.442\n",
      "[0,   220] loss: 3.355\n",
      "[0,   230] loss: 3.314\n",
      "[0,   240] loss: 3.204\n",
      "[0,   250] loss: 3.211\n",
      "[0,   260] loss: 3.066\n",
      "[0,   270] loss: 3.009\n",
      "[0,   280] loss: 2.885\n",
      "[0,   290] loss: 2.852\n",
      "[0,   300] loss: 2.809\n",
      "[1,    10] loss: 2.521\n",
      "[1,    20] loss: 2.583\n",
      "[1,    30] loss: 2.500\n",
      "[1,    40] loss: 2.453\n",
      "[1,    50] loss: 2.407\n",
      "[1,    60] loss: 2.373\n",
      "[1,    70] loss: 2.319\n",
      "[1,    80] loss: 2.308\n",
      "[1,    90] loss: 2.236\n",
      "[1,   100] loss: 2.237\n",
      "[1,   110] loss: 2.214\n",
      "[1,   120] loss: 2.131\n",
      "[1,   130] loss: 2.092\n",
      "[1,   140] loss: 2.097\n",
      "[1,   150] loss: 1.983\n",
      "[1,   160] loss: 1.984\n",
      "[1,   170] loss: 1.974\n",
      "[1,   180] loss: 1.916\n",
      "[1,   190] loss: 1.909\n",
      "[1,   200] loss: 1.937\n",
      "[1,   210] loss: 1.947\n",
      "[1,   220] loss: 1.858\n",
      "[1,   230] loss: 1.831\n",
      "[1,   240] loss: 1.804\n",
      "[1,   250] loss: 1.821\n",
      "[1,   260] loss: 1.798\n",
      "[1,   270] loss: 1.783\n",
      "[1,   280] loss: 1.761\n",
      "[1,   290] loss: 1.691\n",
      "[1,   300] loss: 1.675\n",
      "[2,    10] loss: 1.545\n",
      "[2,    20] loss: 1.480\n",
      "[2,    30] loss: 1.438\n",
      "[2,    40] loss: 1.417\n",
      "[2,    50] loss: 1.457\n",
      "[2,    60] loss: 1.488\n",
      "[2,    70] loss: 1.459\n",
      "[2,    80] loss: 1.358\n",
      "[2,    90] loss: 1.365\n",
      "[2,   100] loss: 1.419\n",
      "[2,   110] loss: 1.327\n",
      "[2,   120] loss: 1.326\n",
      "[2,   130] loss: 1.380\n",
      "[2,   140] loss: 1.358\n",
      "[2,   150] loss: 1.357\n",
      "[2,   160] loss: 1.337\n",
      "[2,   170] loss: 1.303\n",
      "[2,   180] loss: 1.264\n",
      "[2,   190] loss: 1.239\n",
      "[2,   200] loss: 1.276\n",
      "[2,   210] loss: 1.328\n",
      "[2,   220] loss: 1.336\n",
      "[2,   230] loss: 1.240\n",
      "[2,   240] loss: 1.272\n",
      "[2,   250] loss: 1.309\n",
      "[2,   260] loss: 1.235\n",
      "[2,   270] loss: 1.278\n",
      "[2,   280] loss: 1.257\n",
      "[2,   290] loss: 1.189\n",
      "[2,   300] loss: 1.248\n",
      "Learning rate: 0.001000\n",
      "[3,    10] loss: 1.038\n",
      "[3,    20] loss: 1.046\n",
      "[3,    30] loss: 0.945\n",
      "[3,    40] loss: 0.941\n",
      "[3,    50] loss: 1.011\n",
      "[3,    60] loss: 0.965\n",
      "[3,    70] loss: 0.961\n",
      "[3,    80] loss: 0.931\n",
      "[3,    90] loss: 0.930\n",
      "[3,   100] loss: 0.882\n",
      "[3,   110] loss: 0.897\n",
      "[3,   120] loss: 0.908\n",
      "[3,   130] loss: 0.897\n",
      "[3,   140] loss: 0.886\n",
      "[3,   150] loss: 0.884\n",
      "[3,   160] loss: 0.850\n",
      "[3,   170] loss: 0.830\n",
      "[3,   180] loss: 0.910\n",
      "[3,   190] loss: 0.823\n",
      "[3,   200] loss: 0.847\n",
      "[3,   210] loss: 0.940\n",
      "[3,   220] loss: 0.916\n",
      "[3,   230] loss: 0.820\n",
      "[3,   240] loss: 0.859\n",
      "[3,   250] loss: 0.861\n",
      "[3,   260] loss: 0.860\n",
      "[3,   270] loss: 0.832\n",
      "[3,   280] loss: 0.837\n",
      "[3,   290] loss: 0.837\n",
      "[3,   300] loss: 0.834\n",
      "[4,    10] loss: 0.773\n",
      "[4,    20] loss: 0.826\n",
      "[4,    30] loss: 0.858\n",
      "[4,    40] loss: 0.801\n",
      "[4,    50] loss: 0.799\n",
      "[4,    60] loss: 0.764\n",
      "[4,    70] loss: 0.817\n",
      "[4,    80] loss: 0.792\n",
      "[4,    90] loss: 0.785\n",
      "[4,   100] loss: 0.819\n",
      "[4,   110] loss: 0.792\n",
      "[4,   120] loss: 0.806\n",
      "[4,   130] loss: 0.776\n",
      "[4,   140] loss: 0.789\n",
      "[4,   150] loss: 0.801\n",
      "[4,   160] loss: 0.816\n",
      "[4,   170] loss: 0.792\n",
      "[4,   180] loss: 0.802\n",
      "[4,   190] loss: 0.778\n",
      "[4,   200] loss: 0.798\n",
      "[4,   210] loss: 0.800\n",
      "[4,   220] loss: 0.814\n",
      "[4,   230] loss: 0.806\n",
      "[4,   240] loss: 0.756\n",
      "[4,   250] loss: 0.739\n",
      "[4,   260] loss: 0.801\n",
      "[4,   270] loss: 0.802\n",
      "[4,   280] loss: 0.771\n",
      "[4,   290] loss: 0.781\n",
      "[4,   300] loss: 0.767\n",
      "[5,    10] loss: 0.713\n",
      "[5,    20] loss: 0.772\n",
      "[5,    30] loss: 0.728\n",
      "[5,    40] loss: 0.784\n",
      "[5,    50] loss: 0.731\n",
      "[5,    60] loss: 0.742\n",
      "[5,    70] loss: 0.744\n",
      "[5,    80] loss: 0.715\n",
      "[5,    90] loss: 0.767\n",
      "[5,   100] loss: 0.739\n",
      "[5,   110] loss: 0.719\n",
      "[5,   120] loss: 0.753\n",
      "[5,   130] loss: 0.748\n",
      "[5,   140] loss: 0.767\n",
      "[5,   150] loss: 0.750\n",
      "[5,   160] loss: 0.700\n",
      "[5,   170] loss: 0.706\n",
      "[5,   180] loss: 0.765\n",
      "[5,   190] loss: 0.732\n",
      "[5,   200] loss: 0.788\n",
      "[5,   210] loss: 0.786\n",
      "[5,   220] loss: 0.718\n",
      "[5,   230] loss: 0.723\n",
      "[5,   240] loss: 0.744\n",
      "[5,   250] loss: 0.731\n",
      "[5,   260] loss: 0.719\n",
      "[5,   270] loss: 0.753\n",
      "[5,   280] loss: 0.725\n",
      "[5,   290] loss: 0.743\n",
      "[5,   300] loss: 0.694\n",
      "Learning rate: 0.000100\n",
      "[6,    10] loss: 0.695\n",
      "[6,    20] loss: 0.675\n",
      "[6,    30] loss: 0.645\n",
      "[6,    40] loss: 0.691\n",
      "[6,    50] loss: 0.695\n",
      "[6,    60] loss: 0.701\n",
      "[6,    70] loss: 0.705\n",
      "[6,    80] loss: 0.709\n",
      "[6,    90] loss: 0.726\n",
      "[6,   100] loss: 0.699\n",
      "[6,   110] loss: 0.725\n",
      "[6,   120] loss: 0.715\n",
      "[6,   130] loss: 0.705\n",
      "[6,   140] loss: 0.709\n",
      "[6,   150] loss: 0.689\n",
      "[6,   160] loss: 0.665\n",
      "[6,   170] loss: 0.649\n",
      "[6,   180] loss: 0.661\n",
      "[6,   190] loss: 0.716\n",
      "[6,   200] loss: 0.730\n",
      "[6,   210] loss: 0.711\n",
      "[6,   220] loss: 0.698\n",
      "[6,   230] loss: 0.706\n",
      "[6,   240] loss: 0.724\n",
      "[6,   250] loss: 0.728\n",
      "[6,   260] loss: 0.724\n",
      "[6,   270] loss: 0.663\n",
      "[6,   280] loss: 0.683\n",
      "[6,   290] loss: 0.685\n",
      "[6,   300] loss: 0.698\n"
     ]
    }
   ],
   "source": [
    "# actual training of the model\n",
    "inceptionnet = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "inceptionnet.fc = nn.Linear(2048, 555) # This will reinitialize the layer as well\n",
    "# state = torch.load(checkpoints + 'checkpoint-5.pkl')\n",
    "\n",
    "losses = train(inceptionnet, data['train'], epochs=7, schedule={0:.01, 3:.001, 6:.0001}, lr=.01, print_every=10,\n",
    "               checkpoint_path=checkpoints)#, state = state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5b115a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T02:39:26.234349Z",
     "iopub.status.busy": "2023-06-02T02:39:26.234025Z",
     "iopub.status.idle": "2023-06-02T02:48:59.685922Z",
     "shell.execute_reply": "2023-06-02T02:48:59.684702Z"
    },
    "papermill": {
     "duration": 573.476376,
     "end_time": "2023-06-02T02:48:59.688614",
     "exception": false,
     "start_time": "2023-06-02T02:39:26.212238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.010000\n",
      "Learning rate: 0.001000\n",
      "Learning rate: 0.000100\n",
      "[7,    10] loss: 0.666\n",
      "[7,    20] loss: 0.682\n",
      "[7,    30] loss: 0.696\n",
      "[7,    40] loss: 0.691\n",
      "[7,    50] loss: 0.702\n",
      "[7,    60] loss: 0.656\n",
      "[7,    70] loss: 0.632\n",
      "[7,    80] loss: 0.716\n",
      "[7,    90] loss: 0.662\n",
      "[7,   100] loss: 0.728\n",
      "[7,   110] loss: 0.670\n",
      "[7,   120] loss: 0.671\n",
      "[7,   130] loss: 0.645\n",
      "[7,   140] loss: 0.675\n",
      "[7,   150] loss: 0.718\n",
      "[7,   160] loss: 0.666\n",
      "[7,   170] loss: 0.689\n",
      "[7,   180] loss: 0.748\n",
      "[7,   190] loss: 0.670\n",
      "[7,   200] loss: 0.711\n",
      "[7,   210] loss: 0.672\n",
      "[7,   220] loss: 0.671\n",
      "[7,   230] loss: 0.713\n",
      "[7,   240] loss: 0.616\n",
      "[7,   250] loss: 0.702\n",
      "[7,   260] loss: 0.725\n",
      "[7,   270] loss: 0.701\n",
      "[7,   280] loss: 0.710\n",
      "[7,   290] loss: 0.715\n",
      "[7,   300] loss: 0.703\n"
     ]
    }
   ],
   "source": [
    "inceptionnet = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "inceptionnet.fc = nn.Linear(2048, 555) # This will reinitialize the layer as well\n",
    "data = get_bird_data(1)\n",
    "state = torch.load(checkpoints + 'checkpoint-7.pkl')\n",
    "\n",
    "losses = train(inceptionnet, data['train'], epochs=8, schedule={0:.01, 3:.001, 6:.0001}, lr=.01, print_every=10,\n",
    "               checkpoint_path=checkpoints, state = state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01b9c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T02:48:59.791129Z",
     "iopub.status.busy": "2023-06-02T02:48:59.790761Z",
     "iopub.status.idle": "2023-06-02T02:48:59.802266Z",
     "shell.execute_reply": "2023-06-02T02:48:59.801235Z"
    },
    "papermill": {
     "duration": 0.038027,
     "end_time": "2023-06-02T02:48:59.804177",
     "exception": false,
     "start_time": "2023-06-02T02:48:59.766150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(net, dataloader, ofname):\n",
    "    out = open(ofname, 'w')\n",
    "    out.write(\"path,class\\n\")\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader, 0):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "            \n",
    "            \n",
    "            fname, _ = dataloader.dataset.samples[i]\n",
    "            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n",
    "    out.close()\n",
    "    \n",
    "\n",
    "def accuracy(net, dataloader):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader, 0):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            datal: ndarray = np.equal(labels.cpu(), predicted.cpu())\n",
    "            sz = len(datal)\n",
    "            correct += datal.sum()\n",
    "            total += sz\n",
    "    acc = correct/total * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30db5413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T02:48:59.850258Z",
     "iopub.status.busy": "2023-06-02T02:48:59.849976Z",
     "iopub.status.idle": "2023-06-02T02:52:52.122202Z",
     "shell.execute_reply": "2023-06-02T02:52:52.120934Z"
    },
    "papermill": {
     "duration": 232.298335,
     "end_time": "2023-06-02T02:52:52.124993",
     "exception": false,
     "start_time": "2023-06-02T02:48:59.826658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluation section\n",
    "# print(accuracy(inceptionnet, data['train']))\n",
    "predict(inceptionnet, data['test'], \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4823.288329,
   "end_time": "2023-06-02T02:52:55.672897",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-02T01:32:32.384568",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
